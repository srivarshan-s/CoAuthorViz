{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aae727c1",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "831d9e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "from utils import load_sessions, read_session\n",
    "from main import generate_buffer\n",
    "from events import generate_event_seq\n",
    "from summary import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6eb411",
   "metadata": {},
   "source": [
    "# Compute summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15d7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = load_sessions()\n",
    "# sessions = load_sessions()[:10]\n",
    "\n",
    "file_name = []\n",
    "text = []\n",
    "sentence_metrics_list = []\n",
    "api_metrics_list = []\n",
    "\n",
    "err = []\n",
    "\n",
    "for sess in tqdm(sessions):\n",
    "    events = read_session(sess, verbose=0)\n",
    "    try:\n",
    "        text_buffer = generate_buffer(events)\n",
    "    except:\n",
    "        err.append(str(sess.split('/')[-1]) + \" is throwing an error!\")\n",
    "        continue\n",
    "    file_name.append(sess.split('/')[-1])\n",
    "    text.append(text_buffer[-1])\n",
    "    event_seq_dict = generate_event_seq(buffer=text_buffer,\n",
    "                                        events=events)\n",
    "    sentence_metrics, api_metrics = stats(event_seq_dict)\n",
    "    sentence_metrics_list.append(sentence_metrics)\n",
    "    api_metrics_list.append(api_metrics)\n",
    "    \n",
    "for e in err:\n",
    "    print(e)\n",
    "    \n",
    "df = pd.DataFrame()\n",
    "\n",
    "df[\"file_name\"] = file_name\n",
    "df[\"text\"] = text\n",
    "\n",
    "for col in sentence_metrics_list[0]:\n",
    "    df[str(col)] = [x[col] for x in sentence_metrics_list]\n",
    "    \n",
    "for col in api_metrics_list[0]:\n",
    "    df[str(col)] = [x[col] for x in api_metrics_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953689d0",
   "metadata": {},
   "source": [
    "# Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c789b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(num1, num2):\n",
    "    return float(num1 / num2)\n",
    "\n",
    "\n",
    "def add(num1, num2):\n",
    "    return num1 + num2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603811d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-3 : Total Sentences\n",
    "\n",
    "df[\"GPT-3 : Total Sentences\"] = list(map(get_ratio, \n",
    "    df[\"Number of sentences completely authored by GPT-3\"], \n",
    "    df[\"Total number of sentences\"]\n",
    "))\n",
    "\n",
    "df[\"GPT-3 : Total Sentences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac8020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User : Total Sentences\n",
    "\n",
    "df[\"User : Total Sentences\"] = list(map(get_ratio, \n",
    "    df[\"Number of sentences completely authored by the user\"], \n",
    "    df[\"Total number of sentences\"]\n",
    "))\n",
    "\n",
    "df[\"User : Total Sentences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea93aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount of usage of GPT-3 (SD+SE/SA)\n",
    "\n",
    "df[\"Amount of GTP-3 Usage\"] = list(map(get_ratio, \n",
    "    pd.Series(list(map(add, df[\"Number of sentences authored by GPT-3 and user\"], \n",
    "                       df[\"Number of sentences completely authored by GPT-3\"]))), \n",
    "    df[\"Total number of sentences\"]\n",
    "))\n",
    "\n",
    "df[\"Amount of GTP-3 Usage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a2a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type Token Ratio\n",
    "\n",
    "def get_ttr(text):\n",
    "    sentence_tokens = word_tokenize(text)\n",
    "    punctuations = list(string.punctuation)\n",
    "    sentence_tokens_clean = [word for word in sentence_tokens if word not in punctuations]\n",
    "    ttr = len(set(sentence_tokens_clean)) / len(sentence_tokens_clean)\n",
    "    return ttr\n",
    "\n",
    "\n",
    "df[\"Type Token Ratio\"] = df[\"text\"].apply(get_ttr)\n",
    "df[\"Type Token Ratio\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51886888",
   "metadata": {},
   "source": [
    "# View DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789944a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d391f85",
   "metadata": {},
   "source": [
    "# Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c47157",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"coauthorviz_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71306399",
   "metadata": {},
   "source": [
    "# Read from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "196c4b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>Total number of sentences</th>\n",
       "      <th>Number of sentences of initial prompt</th>\n",
       "      <th>Number of sentences completely authored by the user</th>\n",
       "      <th>Number of sentences completely authored by GPT-3</th>\n",
       "      <th>Number of sentences authored by GPT-3 and user</th>\n",
       "      <th>Total number of GPT-3 calls made</th>\n",
       "      <th>Number of times GPT-3 suggestion is used</th>\n",
       "      <th>Number of times user rejected GPT-3 suggestion</th>\n",
       "      <th>Number of times GPT-3 suggestion is modified</th>\n",
       "      <th>Number of times GPT-3 suggestion is used as is</th>\n",
       "      <th>GPT-3 : Total Sentences</th>\n",
       "      <th>User : Total Sentences</th>\n",
       "      <th>Amount of GTP-3 Usage</th>\n",
       "      <th>Type Token Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8c11358444974bf0b5224183acd8149d.jsonl</td>\n",
       "      <td>What Stereotypical Characters Make You Cringe?...</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.564706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c7dc5563ed07478f9284190b6085f4d3.jsonl</td>\n",
       "      <td>How Worried Should We Be About Screen Time Dur...</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.613559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05a000131fc642f7bb20b62bb20a326e.jsonl</td>\n",
       "      <td>All of the \"#1 Dad\" mugs in the world change t...</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.548476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7834dec912b34643afb92b7c3648a3fe.jsonl</td>\n",
       "      <td>When you die, you appear in a cinema with a nu...</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.469974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105bf88bb4bc42688e06a54644e2989b.jsonl</td>\n",
       "      <td>When you're 28, science discovers a drug that ...</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.270270</td>\n",
       "      <td>0.491135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                file_name  \\\n",
       "0  8c11358444974bf0b5224183acd8149d.jsonl   \n",
       "1  c7dc5563ed07478f9284190b6085f4d3.jsonl   \n",
       "2  05a000131fc642f7bb20b62bb20a326e.jsonl   \n",
       "3  7834dec912b34643afb92b7c3648a3fe.jsonl   \n",
       "4  105bf88bb4bc42688e06a54644e2989b.jsonl   \n",
       "\n",
       "                                                text  \\\n",
       "0  What Stereotypical Characters Make You Cringe?...   \n",
       "1  How Worried Should We Be About Screen Time Dur...   \n",
       "2  All of the \"#1 Dad\" mugs in the world change t...   \n",
       "3  When you die, you appear in a cinema with a nu...   \n",
       "4  When you're 28, science discovers a drug that ...   \n",
       "\n",
       "   Total number of sentences  Number of sentences of initial prompt  \\\n",
       "0                         15                                      4   \n",
       "1                         20                                      6   \n",
       "2                         22                                      1   \n",
       "3                         32                                      2   \n",
       "4                         37                                      3   \n",
       "\n",
       "   Number of sentences completely authored by the user  \\\n",
       "0                                                  6     \n",
       "1                                                 10     \n",
       "2                                                 18     \n",
       "3                                                 18     \n",
       "4                                                 24     \n",
       "\n",
       "   Number of sentences completely authored by GPT-3  \\\n",
       "0                                                 0   \n",
       "1                                                 0   \n",
       "2                                                 0   \n",
       "3                                                 1   \n",
       "4                                                 0   \n",
       "\n",
       "   Number of sentences authored by GPT-3 and user  \\\n",
       "0                                               5   \n",
       "1                                               4   \n",
       "2                                               3   \n",
       "3                                              11   \n",
       "4                                              10   \n",
       "\n",
       "   Total number of GPT-3 calls made  Number of times GPT-3 suggestion is used  \\\n",
       "0                                 5                                         5   \n",
       "1                                 6                                         4   \n",
       "2                                 6                                         3   \n",
       "3                                12                                        12   \n",
       "4                                13                                        10   \n",
       "\n",
       "   Number of times user rejected GPT-3 suggestion  \\\n",
       "0                                               0   \n",
       "1                                               2   \n",
       "2                                               3   \n",
       "3                                               0   \n",
       "4                                               3   \n",
       "\n",
       "   Number of times GPT-3 suggestion is modified  \\\n",
       "0                                             3   \n",
       "1                                             3   \n",
       "2                                             3   \n",
       "3                                             0   \n",
       "4                                             0   \n",
       "\n",
       "   Number of times GPT-3 suggestion is used as is  GPT-3 : Total Sentences  \\\n",
       "0                                               2                  0.00000   \n",
       "1                                               1                  0.00000   \n",
       "2                                               0                  0.00000   \n",
       "3                                              12                  0.03125   \n",
       "4                                              10                  0.00000   \n",
       "\n",
       "   User : Total Sentences  Amount of GTP-3 Usage  Type Token Ratio  \n",
       "0                0.400000               0.333333          0.564706  \n",
       "1                0.500000               0.200000          0.613559  \n",
       "2                0.818182               0.136364          0.548476  \n",
       "3                0.562500               0.375000          0.469974  \n",
       "4                0.648649               0.270270          0.491135  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"coauthorviz_metrics.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bf1946",
   "metadata": {},
   "source": [
    "# Write Sentences to Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3d0219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file_name, text in zip(df[\"file_name\"], df[\"text\"]):\n",
    "#     file_name = file_name.split('.')[0] # Extract only the name and not the 'jsonl' part\n",
    "#     file_name = \"taaco-input-texts/\" + file_name + \".txt\"\n",
    "#     with open(file_name, \"w\") as f:\n",
    "#         f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d44c51",
   "metadata": {},
   "source": [
    "# Read TAACO Metrics CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c74e432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>lemma_ttr</th>\n",
       "      <th>lemma_mattr</th>\n",
       "      <th>lexical_density_tokens</th>\n",
       "      <th>lexical_density_types</th>\n",
       "      <th>content_ttr</th>\n",
       "      <th>function_ttr</th>\n",
       "      <th>function_mattr</th>\n",
       "      <th>noun_ttr</th>\n",
       "      <th>verb_ttr</th>\n",
       "      <th>...</th>\n",
       "      <th>negative_logical</th>\n",
       "      <th>all_temporal</th>\n",
       "      <th>positive_intentional</th>\n",
       "      <th>all_positive</th>\n",
       "      <th>all_negative</th>\n",
       "      <th>all_connective</th>\n",
       "      <th>pronoun_density</th>\n",
       "      <th>pronoun_noun_ratio</th>\n",
       "      <th>repeated_content_lemmas</th>\n",
       "      <th>repeated_content_and_pronoun_lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>007769c9000e457eae8485221041802d.txt</td>\n",
       "      <td>0.378713</td>\n",
       "      <td>0.683549</td>\n",
       "      <td>0.475248</td>\n",
       "      <td>0.751634</td>\n",
       "      <td>0.598958</td>\n",
       "      <td>0.202830</td>\n",
       "      <td>0.405276</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014851</td>\n",
       "      <td>0.014851</td>\n",
       "      <td>0.012376</td>\n",
       "      <td>0.061881</td>\n",
       "      <td>0.014851</td>\n",
       "      <td>0.066832</td>\n",
       "      <td>0.215347</td>\n",
       "      <td>2.023256</td>\n",
       "      <td>0.287129</td>\n",
       "      <td>0.502475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00bf170a815a42359f3aef35f5674ddc.txt</td>\n",
       "      <td>0.471850</td>\n",
       "      <td>0.803765</td>\n",
       "      <td>0.541555</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.653465</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.522951</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.008043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037534</td>\n",
       "      <td>0.008043</td>\n",
       "      <td>0.034853</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.262735</td>\n",
       "      <td>0.313673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00d39011efcb4533ab12076801f74f42.txt</td>\n",
       "      <td>0.326816</td>\n",
       "      <td>0.638641</td>\n",
       "      <td>0.444134</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>0.180905</td>\n",
       "      <td>0.402133</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.587302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>0.053073</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>0.067039</td>\n",
       "      <td>0.148045</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.301676</td>\n",
       "      <td>0.444134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0139e814be15409dbab46c2d2d9ca07f.txt</td>\n",
       "      <td>0.422131</td>\n",
       "      <td>0.753075</td>\n",
       "      <td>0.508197</td>\n",
       "      <td>0.762136</td>\n",
       "      <td>0.633065</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.473822</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>0.010246</td>\n",
       "      <td>0.010246</td>\n",
       "      <td>0.079918</td>\n",
       "      <td>0.010246</td>\n",
       "      <td>0.067623</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>0.566372</td>\n",
       "      <td>0.272541</td>\n",
       "      <td>0.401639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01650a401e614c38a04a904165a5784f.txt</td>\n",
       "      <td>0.503571</td>\n",
       "      <td>0.715758</td>\n",
       "      <td>0.478571</td>\n",
       "      <td>0.709220</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.294521</td>\n",
       "      <td>0.433196</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.010714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.046429</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>0.189286</td>\n",
       "      <td>0.196429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 157 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Filename  lemma_ttr  lemma_mattr  \\\n",
       "0  007769c9000e457eae8485221041802d.txt   0.378713     0.683549   \n",
       "1  00bf170a815a42359f3aef35f5674ddc.txt   0.471850     0.803765   \n",
       "2  00d39011efcb4533ab12076801f74f42.txt   0.326816     0.638641   \n",
       "3  0139e814be15409dbab46c2d2d9ca07f.txt   0.422131     0.753075   \n",
       "4  01650a401e614c38a04a904165a5784f.txt   0.503571     0.715758   \n",
       "\n",
       "   lexical_density_tokens  lexical_density_types  content_ttr  function_ttr  \\\n",
       "0                0.475248               0.751634     0.598958      0.202830   \n",
       "1                0.541555               0.750000     0.653465      0.280702   \n",
       "2                0.444134               0.717949     0.528302      0.180905   \n",
       "3                0.508197               0.762136     0.633065      0.229167   \n",
       "4                0.478571               0.709220     0.746269      0.294521   \n",
       "\n",
       "   function_mattr  noun_ttr  verb_ttr  ...  negative_logical  all_temporal  \\\n",
       "0        0.405276  0.790698  0.500000  ...          0.014851      0.014851   \n",
       "1        0.522951  0.650000  0.603448  ...          0.002681      0.008043   \n",
       "2        0.402133  0.450000  0.587302  ...          0.011173      0.016760   \n",
       "3        0.473822  0.637168  0.571429  ...          0.004098      0.010246   \n",
       "4        0.433196  0.761905  0.666667  ...          0.007143      0.010714   \n",
       "\n",
       "   positive_intentional  all_positive  all_negative  all_connective  \\\n",
       "0              0.012376      0.061881      0.014851        0.066832   \n",
       "1              0.000000      0.037534      0.008043        0.034853   \n",
       "2              0.002793      0.053073      0.011173        0.067039   \n",
       "3              0.010246      0.079918      0.010246        0.067623   \n",
       "4              0.000000      0.053571      0.007143        0.046429   \n",
       "\n",
       "   pronoun_density  pronoun_noun_ratio  repeated_content_lemmas  \\\n",
       "0         0.215347            2.023256                 0.287129   \n",
       "1         0.056300            0.210000                 0.262735   \n",
       "2         0.148045            0.883333                 0.301676   \n",
       "3         0.131148            0.566372                 0.272541   \n",
       "4         0.028571            0.126984                 0.189286   \n",
       "\n",
       "   repeated_content_and_pronoun_lemmas  \n",
       "0                             0.502475  \n",
       "1                             0.313673  \n",
       "2                             0.444134  \n",
       "3                             0.401639  \n",
       "4                             0.196429  \n",
       "\n",
       "[5 rows x 157 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taaco_df = pd.read_csv(\"taaco_results.csv\")\n",
    "taaco_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ef6bb",
   "metadata": {},
   "source": [
    "# Append TAACO Metrics to Original DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09d04c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "taaco_df[\"file_name\"] = taaco_df[\"Filename\"].apply(lambda x: x.split(\".\")[0])\n",
    "taaco_df.drop([\"Filename\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d351fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df\n",
    "new_df[\"file_name\"] = new_df[\"file_name\"].apply(lambda x: x.split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f30f0391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>Total number of sentences</th>\n",
       "      <th>Number of sentences of initial prompt</th>\n",
       "      <th>Number of sentences completely authored by the user</th>\n",
       "      <th>Number of sentences completely authored by GPT-3</th>\n",
       "      <th>Number of sentences authored by GPT-3 and user</th>\n",
       "      <th>Total number of GPT-3 calls made</th>\n",
       "      <th>Number of times GPT-3 suggestion is used</th>\n",
       "      <th>Number of times user rejected GPT-3 suggestion</th>\n",
       "      <th>...</th>\n",
       "      <th>negative_logical</th>\n",
       "      <th>all_temporal</th>\n",
       "      <th>positive_intentional</th>\n",
       "      <th>all_positive</th>\n",
       "      <th>all_negative</th>\n",
       "      <th>all_connective</th>\n",
       "      <th>pronoun_density</th>\n",
       "      <th>pronoun_noun_ratio</th>\n",
       "      <th>repeated_content_lemmas</th>\n",
       "      <th>repeated_content_and_pronoun_lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8c11358444974bf0b5224183acd8149d</td>\n",
       "      <td>What Stereotypical Characters Make You Cringe?...</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>0.011811</td>\n",
       "      <td>0.015748</td>\n",
       "      <td>0.078740</td>\n",
       "      <td>0.023622</td>\n",
       "      <td>0.094488</td>\n",
       "      <td>0.055118</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>0.311024</td>\n",
       "      <td>0.358268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c7dc5563ed07478f9284190b6085f4d3</td>\n",
       "      <td>How Worried Should We Be About Screen Time Dur...</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010204</td>\n",
       "      <td>0.017007</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>0.074830</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>0.088435</td>\n",
       "      <td>0.017007</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>0.224490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05a000131fc642f7bb20b62bb20a326e</td>\n",
       "      <td>All of the \"#1 Dad\" mugs in the world change t...</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.021448</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.061662</td>\n",
       "      <td>0.010724</td>\n",
       "      <td>0.064343</td>\n",
       "      <td>0.045576</td>\n",
       "      <td>0.186813</td>\n",
       "      <td>0.222520</td>\n",
       "      <td>0.265416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7834dec912b34643afb92b7c3648a3fe</td>\n",
       "      <td>When you die, you appear in a cinema with a nu...</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007833</td>\n",
       "      <td>0.028721</td>\n",
       "      <td>0.005222</td>\n",
       "      <td>0.075718</td>\n",
       "      <td>0.007833</td>\n",
       "      <td>0.075718</td>\n",
       "      <td>0.065274</td>\n",
       "      <td>0.409836</td>\n",
       "      <td>0.263708</td>\n",
       "      <td>0.326371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105bf88bb4bc42688e06a54644e2989b</td>\n",
       "      <td>When you're 28, science discovers a drug that ...</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017730</td>\n",
       "      <td>0.019504</td>\n",
       "      <td>0.005319</td>\n",
       "      <td>0.072695</td>\n",
       "      <td>0.019504</td>\n",
       "      <td>0.072695</td>\n",
       "      <td>0.078014</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.241135</td>\n",
       "      <td>0.313830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 172 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          file_name  \\\n",
       "0  8c11358444974bf0b5224183acd8149d   \n",
       "1  c7dc5563ed07478f9284190b6085f4d3   \n",
       "2  05a000131fc642f7bb20b62bb20a326e   \n",
       "3  7834dec912b34643afb92b7c3648a3fe   \n",
       "4  105bf88bb4bc42688e06a54644e2989b   \n",
       "\n",
       "                                                text  \\\n",
       "0  What Stereotypical Characters Make You Cringe?...   \n",
       "1  How Worried Should We Be About Screen Time Dur...   \n",
       "2  All of the \"#1 Dad\" mugs in the world change t...   \n",
       "3  When you die, you appear in a cinema with a nu...   \n",
       "4  When you're 28, science discovers a drug that ...   \n",
       "\n",
       "   Total number of sentences  Number of sentences of initial prompt  \\\n",
       "0                         15                                      4   \n",
       "1                         20                                      6   \n",
       "2                         22                                      1   \n",
       "3                         32                                      2   \n",
       "4                         37                                      3   \n",
       "\n",
       "   Number of sentences completely authored by the user  \\\n",
       "0                                                  6     \n",
       "1                                                 10     \n",
       "2                                                 18     \n",
       "3                                                 18     \n",
       "4                                                 24     \n",
       "\n",
       "   Number of sentences completely authored by GPT-3  \\\n",
       "0                                                 0   \n",
       "1                                                 0   \n",
       "2                                                 0   \n",
       "3                                                 1   \n",
       "4                                                 0   \n",
       "\n",
       "   Number of sentences authored by GPT-3 and user  \\\n",
       "0                                               5   \n",
       "1                                               4   \n",
       "2                                               3   \n",
       "3                                              11   \n",
       "4                                              10   \n",
       "\n",
       "   Total number of GPT-3 calls made  Number of times GPT-3 suggestion is used  \\\n",
       "0                                 5                                         5   \n",
       "1                                 6                                         4   \n",
       "2                                 6                                         3   \n",
       "3                                12                                        12   \n",
       "4                                13                                        10   \n",
       "\n",
       "   Number of times user rejected GPT-3 suggestion  ...  negative_logical  \\\n",
       "0                                               0  ...          0.003937   \n",
       "1                                               2  ...          0.010204   \n",
       "2                                               3  ...          0.002681   \n",
       "3                                               0  ...          0.007833   \n",
       "4                                               3  ...          0.017730   \n",
       "\n",
       "   all_temporal  positive_intentional  all_positive  all_negative  \\\n",
       "0      0.011811              0.015748      0.078740      0.023622   \n",
       "1      0.017007              0.003401      0.074830      0.013605   \n",
       "2      0.021448              0.002681      0.061662      0.010724   \n",
       "3      0.028721              0.005222      0.075718      0.007833   \n",
       "4      0.019504              0.005319      0.072695      0.019504   \n",
       "\n",
       "   all_connective  pronoun_density  pronoun_noun_ratio  \\\n",
       "0        0.094488         0.055118            0.245614   \n",
       "1        0.088435         0.017007            0.058824   \n",
       "2        0.064343         0.045576            0.186813   \n",
       "3        0.075718         0.065274            0.409836   \n",
       "4        0.072695         0.078014            0.478261   \n",
       "\n",
       "   repeated_content_lemmas  repeated_content_and_pronoun_lemmas  \n",
       "0                 0.311024                             0.358268  \n",
       "1                 0.224490                             0.224490  \n",
       "2                 0.222520                             0.265416  \n",
       "3                 0.263708                             0.326371  \n",
       "4                 0.241135                             0.313830  \n",
       "\n",
       "[5 rows x 172 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.merge(new_df, taaco_df, on=\"file_name\")\n",
    "new_df.to_csv(\"all_metrics.csv\")\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cf6149",
   "metadata": {},
   "source": [
    "# Divide into High-usage and Low-usage Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c7e0e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median of Amount of GPT-3 Usage: 0.25\n"
     ]
    }
   ],
   "source": [
    "print(\"Median of Amount of GPT-3 Usage:\", np.median(new_df[\"Amount of GTP-3 Usage\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70cca755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of writing sessions with high GPT-3 usage: 718\n"
     ]
    }
   ],
   "source": [
    "df_high = new_df[new_df[\"Amount of GTP-3 Usage\"] > np.median(new_df[\"Amount of GTP-3 Usage\"])]\n",
    "print(\"Number of writing sessions with high GPT-3 usage:\", len(df_high))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c45d119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of writing sessions with low GPT-3 usage: 728\n"
     ]
    }
   ],
   "source": [
    "df_low = new_df[new_df[\"Amount of GTP-3 Usage\"] <= np.median(new_df[\"Amount of GTP-3 Usage\"])]\n",
    "print(\"Number of writing sessions with low GPT-3 usage:\", len(df_low))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1576b8b",
   "metadata": {},
   "source": [
    "# Print High-usage Group Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81b57577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean lemma_ttr : 0.39611582719532457\n",
      "Mean lemma_mattr : 0.7182977377160918\n",
      "Mean lexical_density_tokens : 0.4926949109058426\n",
      "Mean lexical_density_types : 0.7296625080290305\n",
      "Mean content_ttr : 0.586431661330968\n",
      "Mean function_ttr : 0.2298191084586581\n",
      "Mean function_mattr : 0.4598025822118426\n",
      "Mean noun_ttr : 0.5718294673313705\n",
      "Mean verb_ttr : 0.578616811517585\n",
      "Mean adj_ttr : 0.7458869974028858\n",
      "Mean adv_ttr : 0.650225766476837\n",
      "Mean prp_ttr : 0.17768079241225962\n",
      "Mean argument_ttr : 0.42991633349094843\n",
      "Mean bigram_lemma_ttr : 0.8239844002802577\n",
      "Mean trigram_lemma_ttr : 0.937303613016993\n",
      "Mean adjacent_overlap_all_sent : 0.2209816823547981\n",
      "Mean adjacent_overlap_all_sent_div_seg : 3.093252830333542\n",
      "Mean adjacent_overlap_binary_all_sent : 0.8559451530767535\n",
      "Mean adjacent_overlap_2_all_sent : 0.3255004566188468\n",
      "Mean adjacent_overlap_2_all_sent_div_seg : 4.5227945074852505\n",
      "Mean adjacent_overlap_binary_2_all_sent : 0.9328707260189346\n",
      "Mean adjacent_overlap_cw_sent : 0.1177543106766741\n",
      "Mean adjacent_overlap_cw_sent_div_seg : 0.9113462303415202\n",
      "Mean adjacent_overlap_binary_cw_sent : 0.4937550454177287\n",
      "Mean adjacent_overlap_2_cw_sent : 0.18582667098862243\n",
      "Mean adjacent_overlap_2_cw_sent_div_seg : 1.4263139520624901\n",
      "Mean adjacent_overlap_binary_2_cw_sent : 0.6456451514447368\n",
      "Mean adjacent_overlap_fw_sent : 0.31672808057342616\n",
      "Mean adjacent_overlap_fw_sent_div_seg : 2.072258669151177\n",
      "Mean adjacent_overlap_binary_fw_sent : 0.8103520758669833\n",
      "Mean adjacent_overlap_2_fw_sent : 0.46101288807365176\n",
      "Mean adjacent_overlap_2_fw_sent_div_seg : 2.993680094956894\n",
      "Mean adjacent_overlap_binary_2_fw_sent : 0.9057311934315613\n",
      "Mean adjacent_overlap_noun_sent : 0.1357726468716603\n",
      "Mean adjacent_overlap_noun_sent_div_seg : 0.45517801894806365\n",
      "Mean adjacent_overlap_binary_noun_sent : 0.3285567172435094\n",
      "Mean adjacent_overlap_2_noun_sent : 0.2098808297088801\n",
      "Mean adjacent_overlap_2_noun_sent_div_seg : 0.695727642683189\n",
      "Mean adjacent_overlap_binary_2_noun_sent : 0.45378267775490794\n",
      "Mean adjacent_overlap_verb_sent : 0.11226794017783301\n",
      "Mean adjacent_overlap_verb_sent_div_seg : 0.27415818687174015\n",
      "Mean adjacent_overlap_binary_verb_sent : 0.23139950351386926\n",
      "Mean adjacent_overlap_2_verb_sent : 0.17750710223530183\n",
      "Mean adjacent_overlap_2_verb_sent_div_seg : 0.43222425983330237\n",
      "Mean adjacent_overlap_binary_2_verb_sent : 0.34338604356114233\n",
      "Mean adjacent_overlap_adj_sent : 0.061148375161100284\n",
      "Mean adjacent_overlap_adj_sent_div_seg : 0.07369700171356866\n",
      "Mean adjacent_overlap_binary_adj_sent : 0.06898038418138426\n",
      "Mean adjacent_overlap_2_adj_sent : 0.09924162393963677\n",
      "Mean adjacent_overlap_2_adj_sent_div_seg : 0.12012125667147701\n",
      "Mean adjacent_overlap_binary_2_adj_sent : 0.10719803422024987\n",
      "Mean adjacent_overlap_adv_sent : 0.072729480160039\n",
      "Mean adjacent_overlap_adv_sent_div_seg : 0.07259599269762924\n",
      "Mean adjacent_overlap_binary_adv_sent : 0.06744956849831421\n",
      "Mean adjacent_overlap_2_adv_sent : 0.13068496714511144\n",
      "Mean adjacent_overlap_2_adv_sent_div_seg : 0.12926407401009068\n",
      "Mean adjacent_overlap_binary_2_adv_sent : 0.11885819017784804\n",
      "Mean adjacent_overlap_pronoun_sent : 0.390660900426863\n",
      "Mean adjacent_overlap_pronoun_sent_div_seg : 0.5443445647063748\n",
      "Mean adjacent_overlap_binary_pronoun_sent : 0.45697437219753556\n",
      "Mean adjacent_overlap_2_pronoun_sent : 0.5290979086169564\n",
      "Mean adjacent_overlap_2_pronoun_sent_div_seg : 0.7274210725726202\n",
      "Mean adjacent_overlap_binary_2_pronoun_sent : 0.5677135951676453\n",
      "Mean adjacent_overlap_argument_sent : 0.2216078188370723\n",
      "Mean adjacent_overlap_argument_sent_div_seg : 0.9353528105034932\n",
      "Mean adjacent_overlap_binary_argument_sent : 0.6216890323517437\n",
      "Mean adjacent_overlap_2_argument_sent : 0.31332816717078493\n",
      "Mean adjacent_overlap_2_argument_sent_div_seg : 1.321036165827575\n",
      "Mean adjacent_overlap_binary_2_argument_sent : 0.7489237436239555\n",
      "Mean adjacent_overlap_all_para : 0.25332333055615824\n",
      "Mean adjacent_overlap_all_para_div_seg : 9.182863663319345\n",
      "Mean adjacent_overlap_binary_all_para : 0.8214829489154637\n",
      "Mean adjacent_overlap_2_all_para : 0.38627231101519777\n",
      "Mean adjacent_overlap_2_all_para_div_seg : 12.944496873446727\n",
      "Mean adjacent_overlap_binary_2_all_para : 0.8843670924041519\n",
      "Mean adjacent_overlap_cw_para : 0.1646432463505507\n",
      "Mean adjacent_overlap_cw_para_div_seg : 3.708689576513535\n",
      "Mean adjacent_overlap_binary_cw_para : 0.7189274330261365\n",
      "Mean adjacent_overlap_2_cw_para : 0.27156536148765464\n",
      "Mean adjacent_overlap_2_cw_para_div_seg : 5.550438222580166\n",
      "Mean adjacent_overlap_binary_2_cw_para : 0.8312939083745126\n",
      "Mean adjacent_overlap_fw_para : 0.3770178081500473\n",
      "Mean adjacent_overlap_fw_para_div_seg : 5.542145352740479\n",
      "Mean adjacent_overlap_binary_fw_para : 0.8152147781634524\n",
      "Mean adjacent_overlap_2_fw_para : 0.5476209422844178\n",
      "Mean adjacent_overlap_2_fw_para_div_seg : 7.563693188694819\n",
      "Mean adjacent_overlap_binary_2_fw_para : 0.8827959157215542\n",
      "Mean adjacent_overlap_noun_para : 0.17405316911741742\n",
      "Mean adjacent_overlap_noun_para_div_seg : 1.6199082865473102\n",
      "Mean adjacent_overlap_binary_noun_para : 0.6209973846510851\n",
      "Mean adjacent_overlap_2_noun_para : 0.2745062950423915\n",
      "Mean adjacent_overlap_2_noun_para_div_seg : 2.349260389606277\n",
      "Mean adjacent_overlap_binary_2_noun_para : 0.7536872321864205\n",
      "Mean adjacent_overlap_verb_para : 0.16391965083018661\n",
      "Mean adjacent_overlap_verb_para_div_seg : 1.2032646532029738\n",
      "Mean adjacent_overlap_binary_verb_para : 0.5248143102444681\n",
      "Mean adjacent_overlap_2_verb_para : 0.2810035242166617\n",
      "Mean adjacent_overlap_2_verb_para_div_seg : 1.8623659976979885\n",
      "Mean adjacent_overlap_binary_2_verb_para : 0.687295890244008\n",
      "Mean adjacent_overlap_adj_para : 0.09649108682916227\n",
      "Mean adjacent_overlap_adj_para_div_seg : 0.3563597329681305\n",
      "Mean adjacent_overlap_binary_adj_para : 0.24743000478221353\n",
      "Mean adjacent_overlap_2_adj_para : 0.17303748481069764\n",
      "Mean adjacent_overlap_2_adj_para_div_seg : 0.5528034387005601\n",
      "Mean adjacent_overlap_binary_2_adj_para : 0.3602175770947156\n",
      "Mean adjacent_overlap_adv_para : 0.13326376765198245\n",
      "Mean adjacent_overlap_adv_para_div_seg : 0.4176105056202015\n",
      "Mean adjacent_overlap_binary_adv_para : 0.2661097189073866\n",
      "Mean adjacent_overlap_2_adv_para : 0.23418843892921296\n",
      "Mean adjacent_overlap_2_adv_para_div_seg : 0.6312771511410467\n",
      "Mean adjacent_overlap_binary_2_adv_para : 0.397757735736101\n",
      "Mean adjacent_overlap_pronoun_para : 0.4177085220533294\n",
      "Mean adjacent_overlap_pronoun_para_div_seg : 1.1338504932079734\n",
      "Mean adjacent_overlap_binary_pronoun_para : 0.5852448541319301\n",
      "Mean adjacent_overlap_2_pronoun_para : 0.5897119001593297\n",
      "Mean adjacent_overlap_2_pronoun_para_div_seg : 1.4830363478529596\n",
      "Mean adjacent_overlap_binary_2_pronoun_para : 0.6976458368294736\n",
      "Mean adjacent_overlap_argument_para : 0.2260739510693847\n",
      "Mean adjacent_overlap_argument_para_div_seg : 2.518270169783557\n",
      "Mean adjacent_overlap_binary_argument_para : 0.7316884921609413\n",
      "Mean adjacent_overlap_2_argument_para : 0.33832646535919636\n",
      "Mean adjacent_overlap_2_argument_para_div_seg : 3.5127838649230974\n",
      "Mean adjacent_overlap_binary_2_argument_para : 0.8285004333699222\n",
      "Mean lsa_1_all_sent : 0.3124784602631205\n",
      "Mean lsa_2_all_sent : 0.6913205320446156\n",
      "Mean lsa_1_all_para : 0.4667243624859836\n",
      "Mean lsa_2_all_para : 0.6899630178366671\n",
      "Mean basic_connectives : 0.04188879138728176\n",
      "Mean conjunctions : 0.03410941194213563\n",
      "Mean disjunctions : 0.0038332073903739232\n",
      "Mean lexical_subordinators : 0.018780511198894054\n",
      "Mean coordinating_conjuncts : 0.00442665721127117\n",
      "Mean addition : 0.03411445928920656\n",
      "Mean sentence_linking : 0.025270115857484596\n",
      "Mean order : 0.004646883021790487\n",
      "Mean reason_and_purpose : 0.009264456950035598\n",
      "Mean all_causal : 0.01044487627773904\n",
      "Mean positive_causal : 0.019913677625888467\n",
      "Mean opposition : 0.008431305034598635\n",
      "Mean determiners : 0.10119831403814973\n",
      "Mean all_demonstratives : 0.02244347669930252\n",
      "Mean attended_demonstratives : 0.004788815026917117\n",
      "Mean unattended_demonstratives : 0.01767290557189224\n",
      "Mean all_additive : 0.04792020719644735\n",
      "Mean all_logical : 0.03499985782425197\n",
      "Mean positive_logical : 0.01641357659452167\n",
      "Mean negative_logical : 0.0074188074280227435\n",
      "Mean all_temporal : 0.013116795479033928\n",
      "Mean positive_intentional : 0.006012219254534624\n",
      "Mean all_positive : 0.06721392683572786\n",
      "Mean all_negative : 0.011752726806400335\n",
      "Mean all_connective : 0.0682881357641631\n",
      "Mean pronoun_density : 0.0695435425162579\n",
      "Mean pronoun_noun_ratio : 0.3570260242694809\n",
      "Mean repeated_content_lemmas : 0.28684114679011696\n",
      "Mean repeated_content_and_pronoun_lemmas : 0.3533222524322535\n"
     ]
    }
   ],
   "source": [
    "for col in df_high.columns:\n",
    "    if col in df.columns:\n",
    "        continue\n",
    "    print(\"Mean\", col, \":\", np.mean(df_high[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e516aa",
   "metadata": {},
   "source": [
    "# Print Low-usage Group Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "578feb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean lemma_ttr : 0.40644688188353156\n",
      "Mean lemma_mattr : 0.7358031241203737\n",
      "Mean lexical_density_tokens : 0.4985475465362733\n",
      "Mean lexical_density_types : 0.7250907969085123\n",
      "Mean content_ttr : 0.5914766065546153\n",
      "Mean function_ttr : 0.2428463920135673\n",
      "Mean function_mattr : 0.4814854760680687\n",
      "Mean noun_ttr : 0.582911587011923\n",
      "Mean verb_ttr : 0.575548092035228\n",
      "Mean adj_ttr : 0.7482037402667884\n",
      "Mean adv_ttr : 0.6518250262489066\n",
      "Mean prp_ttr : 0.17899325255223886\n",
      "Mean argument_ttr : 0.4392265208983434\n",
      "Mean bigram_lemma_ttr : 0.8510042890397267\n",
      "Mean trigram_lemma_ttr : 0.955060558344478\n",
      "Mean adjacent_overlap_all_sent : 0.20340912779507023\n",
      "Mean adjacent_overlap_all_sent_div_seg : 2.791734198145305\n",
      "Mean adjacent_overlap_binary_all_sent : 0.8361618813735054\n",
      "Mean adjacent_overlap_2_all_sent : 0.30347431711387773\n",
      "Mean adjacent_overlap_2_all_sent_div_seg : 4.130514578567157\n",
      "Mean adjacent_overlap_binary_2_all_sent : 0.9179715343113309\n",
      "Mean adjacent_overlap_cw_sent : 0.10753751800784793\n",
      "Mean adjacent_overlap_cw_sent_div_seg : 0.8208818073126567\n",
      "Mean adjacent_overlap_binary_cw_sent : 0.45866299916347214\n",
      "Mean adjacent_overlap_2_cw_sent : 0.17012837301187017\n",
      "Mean adjacent_overlap_2_cw_sent_div_seg : 1.2844719505033861\n",
      "Mean adjacent_overlap_binary_2_cw_sent : 0.6079924135953544\n",
      "Mean adjacent_overlap_fw_sent : 0.2899965361208173\n",
      "Mean adjacent_overlap_fw_sent_div_seg : 1.854944655451886\n",
      "Mean adjacent_overlap_binary_fw_sent : 0.7797906576073763\n",
      "Mean adjacent_overlap_2_fw_sent : 0.4294919976585013\n",
      "Mean adjacent_overlap_2_fw_sent_div_seg : 2.725552593819967\n",
      "Mean adjacent_overlap_binary_2_fw_sent : 0.8833687117450933\n",
      "Mean adjacent_overlap_noun_sent : 0.1240613860608117\n",
      "Mean adjacent_overlap_noun_sent_div_seg : 0.4028959726094389\n",
      "Mean adjacent_overlap_binary_noun_sent : 0.29704041936145376\n",
      "Mean adjacent_overlap_2_noun_sent : 0.19067789237777982\n",
      "Mean adjacent_overlap_2_noun_sent_div_seg : 0.6093301336254839\n",
      "Mean adjacent_overlap_binary_2_noun_sent : 0.41482511373468806\n",
      "Mean adjacent_overlap_verb_sent : 0.10261800591127875\n",
      "Mean adjacent_overlap_verb_sent_div_seg : 0.24924108947483087\n",
      "Mean adjacent_overlap_binary_verb_sent : 0.2143809277784411\n",
      "Mean adjacent_overlap_2_verb_sent : 0.16681069383997293\n",
      "Mean adjacent_overlap_2_verb_sent_div_seg : 0.40023207340827854\n",
      "Mean adjacent_overlap_binary_2_verb_sent : 0.32645414984805426\n",
      "Mean adjacent_overlap_adj_sent : 0.05465020169681263\n",
      "Mean adjacent_overlap_adj_sent_div_seg : 0.06255147479219245\n",
      "Mean adjacent_overlap_binary_adj_sent : 0.05792112178080068\n",
      "Mean adjacent_overlap_2_adj_sent : 0.08825654333850179\n",
      "Mean adjacent_overlap_2_adj_sent_div_seg : 0.10185987061554451\n",
      "Mean adjacent_overlap_binary_2_adj_sent : 0.0904866541108378\n",
      "Mean adjacent_overlap_adv_sent : 0.06604670125717747\n",
      "Mean adjacent_overlap_adv_sent_div_seg : 0.068733959695943\n",
      "Mean adjacent_overlap_binary_adv_sent : 0.06526948210356044\n",
      "Mean adjacent_overlap_2_adv_sent : 0.12053513860195605\n",
      "Mean adjacent_overlap_2_adv_sent_div_seg : 0.1233288810495566\n",
      "Mean adjacent_overlap_binary_2_adv_sent : 0.11508187260121608\n",
      "Mean adjacent_overlap_pronoun_sent : 0.36429612281326773\n",
      "Mean adjacent_overlap_pronoun_sent_div_seg : 0.5065312384446108\n",
      "Mean adjacent_overlap_binary_pronoun_sent : 0.4285612233798124\n",
      "Mean adjacent_overlap_2_pronoun_sent : 0.5030584619587953\n",
      "Mean adjacent_overlap_2_pronoun_sent_div_seg : 0.6901859053760178\n",
      "Mean adjacent_overlap_binary_2_pronoun_sent : 0.5436875186067842\n",
      "Mean adjacent_overlap_argument_sent : 0.2053213175200705\n",
      "Mean adjacent_overlap_argument_sent_div_seg : 0.8429618197599876\n",
      "Mean adjacent_overlap_binary_argument_sent : 0.5755963017997514\n",
      "Mean adjacent_overlap_2_argument_sent : 0.2919420093812047\n",
      "Mean adjacent_overlap_2_argument_sent_div_seg : 1.1946627601237212\n",
      "Mean adjacent_overlap_binary_2_argument_sent : 0.7078321072845563\n",
      "Mean adjacent_overlap_all_para : 0.2598193713240334\n",
      "Mean adjacent_overlap_all_para_div_seg : 9.457456835169973\n",
      "Mean adjacent_overlap_binary_all_para : 0.8163578568247515\n",
      "Mean adjacent_overlap_2_all_para : 0.40061310327253985\n",
      "Mean adjacent_overlap_2_all_para_div_seg : 12.874461015550468\n",
      "Mean adjacent_overlap_binary_2_all_para : 0.8920642151875959\n",
      "Mean adjacent_overlap_cw_para : 0.16829426397543407\n",
      "Mean adjacent_overlap_cw_para_div_seg : 3.7827695337378806\n",
      "Mean adjacent_overlap_binary_cw_para : 0.730225445685996\n",
      "Mean adjacent_overlap_2_cw_para : 0.28468584042579026\n",
      "Mean adjacent_overlap_2_cw_para_div_seg : 5.452513827208999\n",
      "Mean adjacent_overlap_binary_2_cw_para : 0.8500831876471099\n",
      "Mean adjacent_overlap_fw_para : 0.3854958770237114\n",
      "Mean adjacent_overlap_fw_para_div_seg : 5.734716158989958\n",
      "Mean adjacent_overlap_binary_fw_para : 0.8115385414644848\n",
      "Mean adjacent_overlap_2_fw_para : 0.5620474088323983\n",
      "Mean adjacent_overlap_2_fw_para_div_seg : 7.591478672754903\n",
      "Mean adjacent_overlap_binary_2_fw_para : 0.8904080188198613\n",
      "Mean adjacent_overlap_noun_para : 0.17082512733259148\n",
      "Mean adjacent_overlap_noun_para_div_seg : 1.5725251893291794\n",
      "Mean adjacent_overlap_binary_noun_para : 0.6184507419304814\n",
      "Mean adjacent_overlap_2_noun_para : 0.2836332024616073\n",
      "Mean adjacent_overlap_2_noun_para_div_seg : 2.2449912261763116\n",
      "Mean adjacent_overlap_binary_2_noun_para : 0.7647484202590414\n",
      "Mean adjacent_overlap_verb_para : 0.17393063624242047\n",
      "Mean adjacent_overlap_verb_para_div_seg : 1.2948202750420568\n",
      "Mean adjacent_overlap_binary_verb_para : 0.5623137711656313\n",
      "Mean adjacent_overlap_2_verb_para : 0.3021036182434185\n",
      "Mean adjacent_overlap_2_verb_para_div_seg : 1.8880667152549713\n",
      "Mean adjacent_overlap_binary_2_verb_para : 0.7234792753872719\n",
      "Mean adjacent_overlap_adj_para : 0.09449829420019562\n",
      "Mean adjacent_overlap_adj_para_div_seg : 0.33922768854743546\n",
      "Mean adjacent_overlap_binary_adj_para : 0.23579366546974628\n",
      "Mean adjacent_overlap_2_adj_para : 0.17227383408283803\n",
      "Mean adjacent_overlap_2_adj_para_div_seg : 0.5140343681214978\n",
      "Mean adjacent_overlap_binary_2_adj_para : 0.35106214061208446\n",
      "Mean adjacent_overlap_adv_para : 0.14377270144570548\n",
      "Mean adjacent_overlap_adv_para_div_seg : 0.43995175136409576\n",
      "Mean adjacent_overlap_binary_adv_para : 0.30213923713997803\n",
      "Mean adjacent_overlap_2_adv_para : 0.24889364211787607\n",
      "Mean adjacent_overlap_2_adv_para_div_seg : 0.6342175885521683\n",
      "Mean adjacent_overlap_binary_2_adv_para : 0.4255198693898262\n",
      "Mean adjacent_overlap_pronoun_para : 0.44386869329961787\n",
      "Mean adjacent_overlap_pronoun_para_div_seg : 1.246574857359511\n",
      "Mean adjacent_overlap_binary_pronoun_para : 0.6161517552712132\n",
      "Mean adjacent_overlap_2_pronoun_para : 0.6133266081730695\n",
      "Mean adjacent_overlap_2_pronoun_para_div_seg : 1.5708321498458215\n",
      "Mean adjacent_overlap_binary_2_pronoun_para : 0.722476921322007\n",
      "Mean adjacent_overlap_argument_para : 0.22799864494469188\n",
      "Mean adjacent_overlap_argument_para_div_seg : 2.5425893525219783\n",
      "Mean adjacent_overlap_binary_argument_para : 0.7299484275330234\n",
      "Mean adjacent_overlap_2_argument_para : 0.3520655037749537\n",
      "Mean adjacent_overlap_2_argument_para_div_seg : 3.4618628514827336\n",
      "Mean adjacent_overlap_binary_2_argument_para : 0.8354745584074286\n",
      "Mean lsa_1_all_sent : 0.3070360445885295\n",
      "Mean lsa_2_all_sent : 0.6870378640510784\n",
      "Mean lsa_1_all_para : 0.4886254790834562\n",
      "Mean lsa_2_all_para : 0.7082882123021017\n",
      "Mean basic_connectives : 0.039431608140635455\n",
      "Mean conjunctions : 0.03108343969458824\n",
      "Mean disjunctions : 0.0043270268450889705\n",
      "Mean lexical_subordinators : 0.020347512245842324\n",
      "Mean coordinating_conjuncts : 0.004689016145591924\n",
      "Mean addition : 0.03194342521118162\n",
      "Mean sentence_linking : 0.027016242604154146\n",
      "Mean order : 0.005108060200473292\n",
      "Mean reason_and_purpose : 0.009589132529956801\n",
      "Mean all_causal : 0.010827936612803504\n",
      "Mean positive_causal : 0.02144869368584702\n",
      "Mean opposition : 0.008130153428749122\n",
      "Mean determiners : 0.09812039909956223\n",
      "Mean all_demonstratives : 0.02494324275389846\n",
      "Mean attended_demonstratives : 0.005437579261456882\n",
      "Mean unattended_demonstratives : 0.019538551673549752\n",
      "Mean all_additive : 0.045888021713306866\n",
      "Mean all_logical : 0.037651961050949465\n",
      "Mean positive_logical : 0.01854715577376658\n",
      "Mean negative_logical : 0.007172681818388818\n",
      "Mean all_temporal : 0.014538297197199561\n",
      "Mean positive_intentional : 0.005911528453959712\n",
      "Mean all_positive : 0.06740337349283831\n",
      "Mean all_negative : 0.011954792082682844\n",
      "Mean all_connective : 0.06871542781821263\n",
      "Mean pronoun_density : 0.06813561972212694\n",
      "Mean pronoun_noun_ratio : 0.35325720817097916\n",
      "Mean repeated_content_lemmas : 0.2886795393069258\n",
      "Mean repeated_content_and_pronoun_lemmas : 0.3538639522233132\n"
     ]
    }
   ],
   "source": [
    "for col in df_low.columns:\n",
    "    if col in df.columns:\n",
    "        continue\n",
    "    print(\"Mean\", col, \":\", np.mean(df_low[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f3d7bd",
   "metadata": {},
   "source": [
    "# Generate Mean, Median and Standard Deviation for all Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c428ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Median</th>\n",
       "      <th>Standard Deviation</th>\n",
       "      <th>Minimum</th>\n",
       "      <th>Maximum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total number of sentences</td>\n",
       "      <td>28.962656</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>10.388910</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>78.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Number of sentences of initial prompt</td>\n",
       "      <td>4.421162</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.390986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Number of sentences completely authored by the...</td>\n",
       "      <td>16.242739</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>9.535179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number of sentences completely authored by GPT-3</td>\n",
       "      <td>0.685339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.886442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number of sentences authored by GPT-3 and user</td>\n",
       "      <td>7.613416</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.953073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>all_connective</td>\n",
       "      <td>0.068503</td>\n",
       "      <td>0.067545</td>\n",
       "      <td>0.017166</td>\n",
       "      <td>0.017182</td>\n",
       "      <td>0.131078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>pronoun_density</td>\n",
       "      <td>0.068835</td>\n",
       "      <td>0.062363</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.008086</td>\n",
       "      <td>0.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>pronoun_noun_ratio</td>\n",
       "      <td>0.355129</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.230119</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>2.023256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>repeated_content_lemmas</td>\n",
       "      <td>0.287767</td>\n",
       "      <td>0.290206</td>\n",
       "      <td>0.045190</td>\n",
       "      <td>0.138996</td>\n",
       "      <td>0.436620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>repeated_content_and_pronoun_lemmas</td>\n",
       "      <td>0.353595</td>\n",
       "      <td>0.353743</td>\n",
       "      <td>0.054659</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>0.512150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Metrics       Mean     Median  \\\n",
       "0                            Total number of sentences  28.962656  27.000000   \n",
       "1                Number of sentences of initial prompt   4.421162   4.000000   \n",
       "2    Number of sentences completely authored by the...  16.242739  15.000000   \n",
       "3     Number of sentences completely authored by GPT-3   0.685339   0.000000   \n",
       "4       Number of sentences authored by GPT-3 and user   7.613416   6.000000   \n",
       "..                                                 ...        ...        ...   \n",
       "165                                     all_connective   0.068503   0.067545   \n",
       "166                                    pronoun_density   0.068835   0.062363   \n",
       "167                                 pronoun_noun_ratio   0.355129   0.300000   \n",
       "168                            repeated_content_lemmas   0.287767   0.290206   \n",
       "169                repeated_content_and_pronoun_lemmas   0.353595   0.353743   \n",
       "\n",
       "     Standard Deviation    Minimum    Maximum  \n",
       "0             10.388910  11.000000  78.000000  \n",
       "1              2.390986   0.000000   9.000000  \n",
       "2              9.535179   0.000000  64.000000  \n",
       "3              1.886442   0.000000  22.000000  \n",
       "4              5.953073   0.000000  42.000000  \n",
       "..                  ...        ...        ...  \n",
       "165            0.017166   0.017182   0.131078  \n",
       "166            0.032258   0.008086   0.224000  \n",
       "167            0.230119   0.031250   2.023256  \n",
       "168            0.045190   0.138996   0.436620  \n",
       "169            0.054659   0.186667   0.512150  \n",
       "\n",
       "[170 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics_stats_df = pd.DataFrame()\n",
    "\n",
    "metrics = []\n",
    "mean = []\n",
    "median = []\n",
    "std = []\n",
    "minimum = []\n",
    "maximum = []\n",
    "\n",
    "for col in new_df.columns:\n",
    "    if col in [\"file_name\", \"text\"]: continue\n",
    "    metrics.append(col)\n",
    "    mean.append(np.mean(new_df[col]))\n",
    "    median.append(np.median(new_df[col]))\n",
    "    std.append(np.std(new_df[col]))\n",
    "    minimum.append(np.min(new_df[col]))\n",
    "    maximum.append(np.max(new_df[col]))\n",
    "\n",
    "all_metrics_stats_df[\"Metrics\"] = metrics\n",
    "all_metrics_stats_df[\"Mean\"] = mean\n",
    "all_metrics_stats_df[\"Median\"] = median\n",
    "all_metrics_stats_df[\"Standard Deviation\"] = std\n",
    "all_metrics_stats_df[\"Minimum\"] = minimum\n",
    "all_metrics_stats_df[\"Maximum\"] = maximum\n",
    "\n",
    "all_metrics_stats_df.to_csv(\"all_metric_stats.csv\")\n",
    "all_metrics_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f692d01a",
   "metadata": {},
   "source": [
    "# T-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "583ac331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Mean (High Group)</th>\n",
       "      <th>Mean (Low Group)</th>\n",
       "      <th>STD (High Group)</th>\n",
       "      <th>STD (Low Group)</th>\n",
       "      <th>T-Statistic</th>\n",
       "      <th>P-Value</th>\n",
       "      <th>Verdict (alpha=0.05)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lemma_ttr</td>\n",
       "      <td>0.396116</td>\n",
       "      <td>0.406447</td>\n",
       "      <td>0.056508</td>\n",
       "      <td>0.052732</td>\n",
       "      <td>3.592373</td>\n",
       "      <td>3.386763e-04</td>\n",
       "      <td>Means of both groups are different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lemma_mattr</td>\n",
       "      <td>0.718298</td>\n",
       "      <td>0.735803</td>\n",
       "      <td>0.042398</td>\n",
       "      <td>0.034292</td>\n",
       "      <td>8.631911</td>\n",
       "      <td>1.566870e-17</td>\n",
       "      <td>Means of both groups are different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lexical_density_tokens</td>\n",
       "      <td>0.492695</td>\n",
       "      <td>0.498548</td>\n",
       "      <td>0.040122</td>\n",
       "      <td>0.036249</td>\n",
       "      <td>2.909302</td>\n",
       "      <td>3.677817e-03</td>\n",
       "      <td>Means of both groups are different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lexical_density_types</td>\n",
       "      <td>0.729663</td>\n",
       "      <td>0.725091</td>\n",
       "      <td>0.039171</td>\n",
       "      <td>0.035628</td>\n",
       "      <td>-2.320651</td>\n",
       "      <td>2.044443e-02</td>\n",
       "      <td>Means of both groups are different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>content_ttr</td>\n",
       "      <td>0.586432</td>\n",
       "      <td>0.591477</td>\n",
       "      <td>0.074042</td>\n",
       "      <td>0.073001</td>\n",
       "      <td>1.303750</td>\n",
       "      <td>1.925266e-01</td>\n",
       "      <td>Means of both groups are same</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Metric  Mean (High Group)  Mean (Low Group)  \\\n",
       "0               lemma_ttr           0.396116          0.406447   \n",
       "1             lemma_mattr           0.718298          0.735803   \n",
       "2  lexical_density_tokens           0.492695          0.498548   \n",
       "3   lexical_density_types           0.729663          0.725091   \n",
       "4             content_ttr           0.586432          0.591477   \n",
       "\n",
       "   STD (High Group)  STD (Low Group)  T-Statistic       P-Value  \\\n",
       "0          0.056508         0.052732     3.592373  3.386763e-04   \n",
       "1          0.042398         0.034292     8.631911  1.566870e-17   \n",
       "2          0.040122         0.036249     2.909302  3.677817e-03   \n",
       "3          0.039171         0.035628    -2.320651  2.044443e-02   \n",
       "4          0.074042         0.073001     1.303750  1.925266e-01   \n",
       "\n",
       "                 Verdict (alpha=0.05)  \n",
       "0  Means of both groups are different  \n",
       "1  Means of both groups are different  \n",
       "2  Means of both groups are different  \n",
       "3  Means of both groups are different  \n",
       "4       Means of both groups are same  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_test_df = pd.DataFrame()\n",
    "\n",
    "t_stat = []\n",
    "p_val = []\n",
    "metrics = []\n",
    "\n",
    "mean_low_group = []\n",
    "mean_high_group = []\n",
    "\n",
    "std_low_group = []\n",
    "std_high_group = []\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "for col in new_df.columns:\n",
    "    if col in df.columns:\n",
    "        continue\n",
    "    metrics.append(col)\n",
    "    t_stat_result, p_val_result = ttest_ind(df_low[col], df_high[col], equal_var=True)\n",
    "    t_stat.append(t_stat_result)\n",
    "    p_val.append(p_val_result)\n",
    "    mean_low_group.append(np.mean(df_low[col]))\n",
    "    mean_high_group.append(np.mean(df_high[col]))\n",
    "    std_low_group.append(np.std(df_low[col]))\n",
    "    std_high_group.append(np.std(df_high[col]))\n",
    "\n",
    "    \n",
    "t_test_df[\"Metric\"] = metrics\n",
    "t_test_df[\"Mean (High Group)\"] = mean_high_group\n",
    "t_test_df[\"Mean (Low Group)\"] = mean_low_group\n",
    "t_test_df[\"STD (High Group)\"] = std_high_group\n",
    "t_test_df[\"STD (Low Group)\"] = std_low_group\n",
    "t_test_df[\"T-Statistic\"] = t_stat\n",
    "t_test_df[\"P-Value\"] = p_val\n",
    "t_test_df[\"Verdict (alpha=\" + str(ALPHA) + \")\"] = t_test_df[\"P-Value\"].apply(lambda x:\n",
    "    \"Means of both groups are same\" if x > ALPHA else \"Means of both groups are different\")\n",
    "\n",
    "t_test_df.to_csv(\"t_test_results.csv\")\n",
    "t_test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01be6c0",
   "metadata": {},
   "source": [
    "# Check if the Groups are normally distributed using Shapiro-Wilk Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fbb855b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Statistic (Low Group)</th>\n",
       "      <th>P-Value (Low Group)</th>\n",
       "      <th>Distribution (Low Group)</th>\n",
       "      <th>Statistic (High Group)</th>\n",
       "      <th>P-Value (High Group)</th>\n",
       "      <th>Distribution (High Group)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lemma_ttr</td>\n",
       "      <td>0.998517</td>\n",
       "      <td>0.815840</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.998517</td>\n",
       "      <td>0.815840</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lemma_mattr</td>\n",
       "      <td>0.990457</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>Not Normal</td>\n",
       "      <td>0.990457</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lexical_density_tokens</td>\n",
       "      <td>0.996504</td>\n",
       "      <td>0.115982</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.996504</td>\n",
       "      <td>0.115982</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lexical_density_types</td>\n",
       "      <td>0.987280</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>Not Normal</td>\n",
       "      <td>0.987280</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>content_ttr</td>\n",
       "      <td>0.999318</td>\n",
       "      <td>0.997426</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.999318</td>\n",
       "      <td>0.997426</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Metric  Statistic (Low Group)  P-Value (Low Group)  \\\n",
       "0               lemma_ttr               0.998517             0.815840   \n",
       "1             lemma_mattr               0.990457             0.000132   \n",
       "2  lexical_density_tokens               0.996504             0.115982   \n",
       "3   lexical_density_types               0.987280             0.000007   \n",
       "4             content_ttr               0.999318             0.997426   \n",
       "\n",
       "  Distribution (Low Group)  Statistic (High Group)  P-Value (High Group)  \\\n",
       "0                   Normal                0.998517              0.815840   \n",
       "1               Not Normal                0.990457              0.000132   \n",
       "2                   Normal                0.996504              0.115982   \n",
       "3               Not Normal                0.987280              0.000007   \n",
       "4                   Normal                0.999318              0.997426   \n",
       "\n",
       "  Distribution (High Group)  \n",
       "0                    Normal  \n",
       "1                Not Normal  \n",
       "2                    Normal  \n",
       "3                Not Normal  \n",
       "4                    Normal  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapiro_df = pd.DataFrame()\n",
    "\n",
    "stat_low = []\n",
    "stat_high = []\n",
    "p_val_low = []\n",
    "p_val_high = []\n",
    "metrics = []\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "for col in new_df.columns:\n",
    "    if col in df.columns:\n",
    "        continue\n",
    "    metrics.append(col)\n",
    "    stat_result_low, p_val_result_low = shapiro(df_high[col])\n",
    "    stat_low.append(stat_result_low)\n",
    "    p_val_low.append(p_val_result_low)\n",
    "    stat_result_high, p_val_result_high = shapiro(df_high[col])\n",
    "    stat_high.append(stat_result_high)\n",
    "    p_val_high.append(p_val_result_high)\n",
    "    \n",
    "shapiro_df[\"Metric\"] = metrics\n",
    "\n",
    "shapiro_df[\"Statistic (Low Group)\"] = stat_low\n",
    "shapiro_df[\"P-Value (Low Group)\"] = p_val_low\n",
    "shapiro_df[\"Distribution (Low Group)\"] = shapiro_df[\"P-Value (Low Group)\"].apply(\n",
    "    lambda x: \"Normal\" if x > ALPHA else \"Not Normal\")\n",
    "\n",
    "shapiro_df[\"Statistic (High Group)\"] = stat_high\n",
    "shapiro_df[\"P-Value (High Group)\"] = p_val_high\n",
    "shapiro_df[\"Distribution (High Group)\"] = shapiro_df[\"P-Value (High Group)\"].apply(\n",
    "    lambda x: \"Normal\" if x > ALPHA else \"Not Normal\")\n",
    "\n",
    "shapiro_df.to_csv(\"shapiro_wilk_test_results.csv\")\n",
    "shapiro_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b541a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Statistic (Low Group)</th>\n",
       "      <th>P-Value (Low Group)</th>\n",
       "      <th>Distribution (Low Group)</th>\n",
       "      <th>Statistic (High Group)</th>\n",
       "      <th>P-Value (High Group)</th>\n",
       "      <th>Distribution (High Group)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lemma_ttr</td>\n",
       "      <td>0.998517</td>\n",
       "      <td>8.158395e-01</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.998517</td>\n",
       "      <td>8.158395e-01</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>adjacent_overlap_all_sent</td>\n",
       "      <td>0.985989</td>\n",
       "      <td>2.261002e-06</td>\n",
       "      <td>Not Normal</td>\n",
       "      <td>0.985989</td>\n",
       "      <td>2.261002e-06</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>adjacent_overlap_all_para</td>\n",
       "      <td>0.938885</td>\n",
       "      <td>1.381758e-16</td>\n",
       "      <td>Not Normal</td>\n",
       "      <td>0.938885</td>\n",
       "      <td>1.381758e-16</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>lsa_1_all_sent</td>\n",
       "      <td>0.989573</td>\n",
       "      <td>5.572468e-05</td>\n",
       "      <td>Not Normal</td>\n",
       "      <td>0.989573</td>\n",
       "      <td>5.572468e-05</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>lsa_1_all_para</td>\n",
       "      <td>0.980973</td>\n",
       "      <td>4.821818e-08</td>\n",
       "      <td>Not Normal</td>\n",
       "      <td>0.980973</td>\n",
       "      <td>4.821818e-08</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Metric  Statistic (Low Group)  P-Value (Low Group)  \\\n",
       "0                    lemma_ttr               0.998517         8.158395e-01   \n",
       "15   adjacent_overlap_all_sent               0.985989         2.261002e-06   \n",
       "69   adjacent_overlap_all_para               0.938885         1.381758e-16   \n",
       "123             lsa_1_all_sent               0.989573         5.572468e-05   \n",
       "125             lsa_1_all_para               0.980973         4.821818e-08   \n",
       "\n",
       "    Distribution (Low Group)  Statistic (High Group)  P-Value (High Group)  \\\n",
       "0                     Normal                0.998517          8.158395e-01   \n",
       "15                Not Normal                0.985989          2.261002e-06   \n",
       "69                Not Normal                0.938885          1.381758e-16   \n",
       "123               Not Normal                0.989573          5.572468e-05   \n",
       "125               Not Normal                0.980973          4.821818e-08   \n",
       "\n",
       "    Distribution (High Group)  \n",
       "0                      Normal  \n",
       "15                 Not Normal  \n",
       "69                 Not Normal  \n",
       "123                Not Normal  \n",
       "125                Not Normal  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req_metrics = [\"lemma_ttr\", \"adjacent_overlap_all_sent\", \"adjacent_overlap_all_para\",\n",
    "    \"lsa_1_all_sent\", \"lsa_1_all_para\", \"all_connective\"]\n",
    "\n",
    "shapiro_df[shapiro_df[\"Metric\"].isin(req_metrics)].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
